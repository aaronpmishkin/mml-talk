\documentclass[usenames,dvipsnames,mathserif,notheorems]{beamer}

% silence annoying warnings
\usepackage{silence}
\usepackage{caption}
\WarningFilter{remreset}{The remreset package}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{centernot}

\input{macros/math}
\input{macros/plots}

\pgfdeclarelayer{ft}
\pgfdeclarelayer{bg}
\pgfsetlayers{bg,main,ft}

\usepackage{simplebeam}
\usetheme{simplebeamer}

\usetikzlibrary{shapes, arrows}
\usetikzlibrary{decorations.pathreplacing, calligraphy}

% node styles
\tikzstyle{Input}=[minimum size=0.3cm, fill=black, line width = 0.5mm, draw=black, shape=circle, text=black]
\tikzstyle{Hidden}=[minimum size=0.3cm, fill=blue, line width = 0.5mm, draw=blue, shape=circle, text=blue]
\tikzstyle{Splits}=[inner sep=0.03cm, minimum size=0.3cm, line width = 0.3mm, draw=blue, shape=circle, text=black]
\tikzstyle{Output}=[minimum size=0.3cm, fill=white, line width = 0.5mm, draw=black, shape=circle, text=black]

% Edge styles
\tikzstyle{arrow}=[line width = 0.5mm]

% bib resources

\addbibresource[]{refs.bib}

\title{Optimal Sets and Solution Paths of ReLU Networks}
%\subtitle{}
\author{Aaron Mishkin \and Mert Pilanci}
\institute{ICML 2023}
\collaborators{
	\includegraphics[width=0.2\linewidth]{assets/flatiron_small.jpeg}
	\includegraphics[width=0.2\linewidth]{assets/mert.jpg}
}

\titlegraphic{\includegraphics[width=0.4\textwidth]{assets/SUSig_2color_Stree_Left.eps}}

\newcommand{\horizontalrule}{
	{
			\vspace{-0.5em}
			\center \rule{\textwidth}{0.1em}
			\vspace{-0.2em}
		}
}

\definecolor{bad}{HTML}{eb6223}
\definecolor{good}{HTML}{9434ed}

\newcommand{\bad}[1]{\textcolor{bad}{#1}}
\newcommand{\good}[1]{\textcolor{good}{#1}}
\newcommand{\purple}[1]{\textcolor{Magenta}{#1}}

% toggle plotting tikz
\def\showtikz{}

%\logo{\includegraphics[height=0.5cm]{assets/Block_S_2_color.eps}}

%\institute{Stanford University}
\date{}

\begin{document}

\maketitle
%% main content starts %%

\begin{frame}{The Problem}

	{
		\large \bad{Problem}: We don't understand the solution space of
		(even shallow) ReLU networks nearly as well as that of \good{GLMs}.
	}

	\pause
	\vspace{0.5em}
	\horizontalrule
	\vspace{0.5em}

	{
		\large
		Consider the Lasso:
	}

	\pause
	\vspace{0.5em}

	\begin{enumerate}
		\item \textbf{Optimal Sets}: we have an exact \good{polyhedral
			      characterization} and simple criteria for \good{uniqueness}
		      (general position) \citep{tibshirani2013unique}.
		      \pause

		\item \textbf{Regularization Paths}: we know the (min-norm) solution
		      path is \good{continuous} and \good{piece-wise linear}
		      \citep{osborne2000new}.
		      \pause

		\item \textbf{Algorithms}: we have efficient algorithms for
		      \good{homotopy} \citep{efron2004least} and computing \good{minimal
			      solutions} \citep{tibshirani2013unique}.
	\end{enumerate}

\end{frame}

\begin{frame}{Challenges from Non-Convexity}
	\begin{center}
		\Large
		Non-convexity makes extensions beyond GLMs \bad{hard}!
	\end{center}

	\begin{figure}[]
		\centering
		\ifdefined\showtikz
			\input{assets/non_convex.tex}
		\else
			\Huge Non-Convex Figure
		\fi
	\end{figure}

	\pause
	\begin{itemize}
		\item \textbf{Optimality Conditions}: Stationarity \( \centernot \implies \) optimality.
		      We have no global optimality criteria and no certificates.

		      \vspace{1em}

		      \pause
		\item \textbf{Mathematical Tools}: We lose most of convex analysis
		      and have to work with Clarke stationary points, etc.
		      \vspace{1em}

		      \pause

		\item \textbf{Unintuitive Phenomena}: Surprising things happen even
		      with toy neural networks!

	\end{itemize}

\end{frame}

\begin{frame}{Example: Discontinuous Paths}
	Consider training a toy neural network:
	\[
		\min_{w_1} \, \half ((w_1 x_1)_+ - y_1)^2 + \half ((w_1 x_2)_+ - y_2)^2 + \lambda |w_1|.
	\]
	\pause

	\begin{center}
		\input{assets/discontinuous_network_init}
	\end{center}

	\begin{center}
		\Large
		\textcolor{white}{Goal: Overcome these problems via convexification..}
	\end{center}

\end{frame}

\begin{frame}{Example: Discontinuous Paths}
	Consider training a toy ReLU network:
	\[
		\min_{w_1} \, \half ((w_1 x_1)_+ - y_1)^2 + \half ((w_1 x_2)_+ - y_2)^2 + \lambda |w_1|.
	\]

	\begin{center}
		\input{assets/discontinuous_network_0}
	\end{center}

	\begin{center}
		\Large
		\textcolor{white}{Goal: Overcome these problems via convexification..}
	\end{center}

\end{frame}

\begin{frame}{Example: Discontinuous Paths}
	Consider training a toy neural network:
	\[
		\min_{w_1} \, \half ((w_1 x_1)_+ - y_1)^2 + \half ((w_1 x_2)_+ - y_2)^2 + \lambda |w_1|.
	\]

	\begin{center}
		\input{assets/discontinuous_network_1}
	\end{center}

	\begin{center}
		\Large
		\textcolor{white}{Goal: Overcome these problems via convexification..}
	\end{center}


\end{frame}

\begin{frame}{Example: Discontinuous Paths}
	Consider training a toy neural network:
	\[
		\min_{w_1} \, \half ((w_1 x_1)_+ - y_1)^2 + \half ((w_1 x_2)_+ - y_2)^2 + \lambda |w_1|.
	\]

	\begin{center}
		\input{assets/discontinuous_network_2}
	\end{center}

	\pause

	\begin{center}
		\Large
		\good{Goal}: Overcome these problems via convexification.
	\end{center}

\end{frame}

\begin{frame}{Our Contributions}

	{
		\large \good{Overall Approach}: leverage convex reformulations
		of ReLU networks \citep{pilanci2020convex} as an analytical tool.
	}

	\pause
	\vspace{0.5em}
	\horizontalrule
	\vspace{0.5em}

	\begin{enumerate}
		\item \textbf{Optimal Sets}: we characterize all optima of the
		      non-convex training objective.\pause
		      \vspace{0.5em}

		\item \textbf{Uniqueness}: we develop simple criteria for ReLU networks
		      to admit unique solutions up permutation/split symmetries. \pause
		      \vspace{0.5em}

		\item \textbf{Optimal Pruning}: we leverage our theory to give a
		      poly-time procedure for computing minimal ReLU networks.
	\end{enumerate}

\end{frame}


\setbeamercolor{background canvas}{bg=LightCyan}
\begin{frame}{}
	\begin{center}
		\huge I. Background on Convex Reformulations
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Convex Reformulations: Flavor of Results}
	\large

	\textbf{Basic Idea}: We start with a \bad{non-convex} optimization problem and derive
	an equivalent \good{convex} program.

	\pause
	\vspace{2em}

	\textbf{Equivalent} means:
	\vspace{0.5em}
	\begin{itemize}
		\item The global minima have the same values: \( p^* = q^* \)
		      \vspace{0.5em}
		\item We can map every global minimum \( u^* \) for one problem into
		      a global minimum \( v^* \) of the other.
		      \vspace{0.5em}

		      \begin{itemize}
			      \item We call this the \good{solution mapping}.
		      \end{itemize}
	\end{itemize}

\end{frame}


\begin{frame}{Convex Reformulations: Two-Layer ReLU Networks}

	{\large \bad{Non-Convex Problem} (NC-ReLU)}
	\[
		\min_{W_1, w_2} \underbrace{\half \norm{\sum_{j=1}^m (X W_{1j})_+ w_{2j} - y}_2^2}_{\text{Squared Error}}
		+ \underbrace{\lambda \sum_{j=1}^m \norm{W_{1j}}_2^2 + |w_{2j}|^2}_{\text{Weight Decay}},
	\]
	where \( \rbr{x}_+ = \max\cbr{x, 0} \) is the ReLU activation.
	\pause

	\begin{figure}[]
		\centering
		\ifdefined\showtikz
			\input{assets/neural_net}
		\else
			\Huge Neural Network Figure
		\fi
	\end{figure}

\end{frame}


\begin{frame}{Aside: ReLU Activation Patterns}

	Each ReLU neuron is active on a half-space:

	\pause

	\begin{figure}[]
		\centering
		\ifdefined\showtikz
			\input{assets/activation_pattern_0}
		\else
			\Huge activation pattern figure
		\fi
	\end{figure}

	\phantom{
		\textbf{Activation Pattern} satisfies \( D_j X W_{1j} = \rbr{X W_{1j}}_+ \)
	}

\end{frame}

\begin{frame}{Aside: ReLU Activation Patterns}

	Each ReLU neuron is active on a half-space:

	\begin{figure}[]
		\centering
		\ifdefined\showtikz
			\input{assets/activation_pattern_1}
		\else
			\Huge activation pattern figure
		\fi
	\end{figure}

	\pause
	\textbf{Activation Pattern} satisfies \( D_j X W_{1j} = \rbr{X W_{1j}}_+ \)

\end{frame}

\begin{frame}{Aside: ReLU Activation Patterns}

	Each ReLU neuron is active on a half-space:

	\begin{figure}[]
		\centering
		\ifdefined\showtikz
			\input{assets/activation_pattern_2}
		\else
			\Huge activation pattern figure
		\fi
	\end{figure}


	\textbf{Activation Pattern} satisfies \( D_j X W_{1j} = \rbr{X W_{1j}}_+ \)

\end{frame}


\begin{frame}{Convex Reformulations: Convex Problem}

	{\large \good{Convex Reformulation} (C-ReLU)} \citep{pilanci2020convex}
	\[
		\begin{aligned}
			\min_{v, w} & \half \norm{\sum_{j=1}^p D_j X (v_j - w_j) - y}_2^2 +
			\lambda \sum_{j=1}^p \norm{v_j}_2 + \norm{w_j}_2                    \\
			            & \hspace{0.2em} \text{s.t. }
			v_j, w_j \in \calK_j := \cbr{w : (2D_j - I) X w \geq 0},
		\end{aligned}
	\]
	where \( D_j = \text{diag}[\mathbbm{1}(X g_j \geq 0)] \).
	\pause

	\begin{figure}[]
		\centering
		\ifdefined\showtikz
			\input{assets/convex_reformulation}
		\else
			\Huge convex reformulation figure
		\fi
	\end{figure}
\end{frame}

\begin{frame}{Convex Reformulations: Hardness}

	\textbf{Result}: if \( m \geq m^* \) for some \( m^* \leq n \),
	then C-ReLU and NC-ReLU are \good{equivalent}
	\citep{pilanci2020convex}.

	\pause
	\horizontalrule

	How ``hard'' is the convex program?
	\pause

	\[
		p = \abs{\cbr{D_j = \text{diag}[\mathbbm{1}(X g_j \geq 0)] : g_j \in \R^d }}
	\]

	\vspace{2em}
	\pause

	The \textbf{convex program} is:
	\vspace{0.5em}
	\begin{itemize}
		\item \bad{Exponential in general}: \( p \in O(r \cdot (\frac{n}{r})^r) \),
		      where \( r = \text{rank}(X) \).
		      \vspace{0.25em}
		      \begin{itemize}
			      \item Bound comes from theory of hyperplane arrangements \citep{winder1966partitions}.
		      \end{itemize}
		      \pause

		      \vspace{0.5em}

		\item Highly \good{structured} --- it's a (constrained) GLM!
	\end{itemize}

	\vspace{1em}
	\pause

	\begin{center}
		\Large
		We exchange one kind of hardness for another.
	\end{center}

\end{frame}

\setbeamercolor{background canvas}{bg=LightCyan}
\begin{frame}{}
	\begin{center}
		\huge II. Optimal Sets
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Characterizing the Optimal Set}

	{
		\large
		{\Large
			Proof Roadmap:
		}
		\vspace{2em}
		\begin{enumerate}
			\large
			\item \pause
			      Characterize solutions to the \good{convex reformulation}
			      using strong duality and KKT conditions.
			      \vspace{1ex}
			      \pause
			\item Extend results to \bad{non-convex} ReLU networks
			      using the solution mapping.
			      \vspace{1ex}
			      \pause
			\item Leverage explicit characterization of the optimal
			      set for \good{new insights and algorithms}.
		\end{enumerate}
	}


\end{frame}

\begin{frame}{Characterizing the Optimal Set}

	{
		\large
		{\Large
			Proof Roadmap:
		}
		\vspace{2em}
		\begin{enumerate}
			\large
			\item \textbf{Characterize solutions to the \good{convex reformulation}
				      using strong duality and KKT conditions.}
			      \vspace{1ex}
			\item Extend results to \bad{non-convex} ReLU networks
			      using the solution mapping.
			      \vspace{1ex}
			\item Leverage explicit characterization of the optimal
			      set for \good{new insights and algorithms}.
		\end{enumerate}
	}

\end{frame}

\begin{frame}{C-ReLU Optimal Set}

	{\raggedright
		\large
		1. Characterize solutions to the \good{convex reformulation}
		using strong duality and KKT conditions.
		\vspace{3ex}
		\pause
	}

	\horizontalrule

	\textbf{C-ReLU Solution Set}:
	\[
		\begin{aligned}
			\solfn(\lambda) & =
			\argmin_{v_i, w_i \in \calK_i} \, \bigg\{
			\half \bigg\|\sum_{D_i \in \tilde \calD} D_i X (v_i - w_i), y\bigg\|_2^2                    \\
			                & \quad \quad + \lambda\sum_{D_i \in \tilde \calD}\norm{v_i}_2+\norm{w_i}_2
			\bigg\}.
		\end{aligned}
	\]

	\pause

	\begin{itemize}
		\item Convex objective + linear constraints \( \implies \) strong duality!
		      \pause
		\item Introduce dual variables \( \rho \) and analyze the KKT conditions.
		      \pause
		\item Define
		      \(
		      \theta =
		      \begin{bmatrix}
			      v_i \\
			      -w_i
		      \end{bmatrix}
		      \)
		      and index \( D_i \)'s from \( 1 \) to \( 2p \).

	\end{itemize}

\end{frame}

\begin{frame}{C-ReLU Optimality Conditions}

	We form the Lagrangian for the convex reformulation:

	\begin{equation*}
		\begin{aligned}
			\calL(\theta, \rho)
			 & = \half \norm{\sum_{i=1}^{2p} D_i X \theta_i - y}_2^2
			+ \lambda\sum_{i=1}^{2p}\norm{\theta_i}_2
			- \sum_{i=1}^{2p} \abr{K_i^\top \rho_i, \theta_i},
		\end{aligned}
	\end{equation*}
	where \( K_{i} = (2D_i - I) X \).

	\pause
	\horizontalrule

	The \good{KKT conditions} are necessary and sufficient for optimality:\pause

	\vspace{1ex}
	\begin{itemize}
		\item Define the \textbf{optimal model fit}: \( \hat y = \sum_{i=1}^{2p} D_i X \theta_i  \).
		      \pause
		\item The Lagrangian is stationary when,
		      \[
			      \underbrace{X^\top D_i (\hat y - y) + K_i^\top \ri}_{q_i}
			      \in \bad{\partial \lambda \norm{\theta_i}_2}.
		      \]
		      \pause
		\item It turns out each ``block correlation'' \( q_i \) is \good{unique} WLOG!
	\end{itemize}

\end{frame}

\begin{frame}{Characterizing the Optimal Set}

	\textbf{Stationary Lagrangian}:
	\[
		X^\top D_i (\hat y - y) + K_i^\top \ri =:
		\bad{q_i \in \partial \lambda \norm{\theta_i}_2}.
	\]

	\pause
	\horizontalrule

	\textbf{Non-zero Blocks}:
	\begin{itemize}
		\item Suppose \( \good{\theta_i \neq 0} \).
		      \pause
		\item Then
		      \(  \nabla_\theta \lambda \norm{\theta_i}_2
		      = \lambda \frac{\theta_i}{\norm{\theta_i}_2} \)
		      and there exists \( \alpha_\bi > 0 \) for which,
		      \[
			      q_i = \lambda \frac{\theta_i}{\norm{\theta_i}_2}
			      \implies \theta_i = \good{\alpha_\bi} q_i.
		      \]
		      \pause
		      \vspace{-1em}
		\item Every optimal \( \theta_i^* \neq 0 \) is collinear with the
		      (unique) \( q_i \) vector.
	\end{itemize}

\end{frame}

\begin{frame}{C-ReLU: the Optimal Set}
	\vspace{-2ex}
	\begin{itemize}
		\item
		      \textbf{Optimal Fit: }       \( \hat y = \sum_{i=1}^{2p} D_i X \theta_i  \).
		      \vspace{1ex}

		\item
		      \textbf{Block Correlation: } \( q_i := X^\top D_i (\hat y - y) + K_i^\top \ri \).
		      \pause
		      \vspace{1ex}

		\item
		      \textbf{Support Set: }      \( \calS_\lambda
		      = \cbr{\bi \in [2p] : \exists \theta \in \solfn(\lambda), \
			      \theta_i \neq 0} \).
	\end{itemize}

	\vspace{-2ex}
	\pause
	\horizontalrule
	\vspace{-1ex}

	\begin{proposition}[Informal]
		Fix \( \lambda > 0 \).
		The optimal set of the C-ReLU problem is
		given by
		\begin{equation*}
			\begin{aligned}
				\solfn(\lambda) =
				\big\{ & \theta  :
				\forall \, \bi  \in  \calS_\lambda,
				\good{\theta_i =  \alpha_\bi q_i}, \alpha_\bi \geq 0, \,      \\
				       & \quad \forall \, j \in [2p] \setminus \calS_\lambda,
				\theta_{j} = 0, \, \good{\sum_{i=1}^{2p} D_i X \theta_i = \hat y}
				\big\}
			\end{aligned}
		\end{equation*}
	\end{proposition}

\end{frame}

\begin{frame}{Returning to our Roadmap}

	{
		\large
		{\Large
			Proof Roadmap:
		}
		\pause
		\vspace{2em}
		\begin{enumerate}
			\large
			\item Characterize solutions to the \good{convex reformulation}
			      using strong duality and KKT conditions.
			      \vspace{1ex}
			\item \textbf{Extend results to \bad{non-convex} ReLU networks
				      using the solution mapping.}
			      \vspace{1ex}
			\item Leverage explicit characterization of the optimal
			      set for \good{new insights and algorithms}.
		\end{enumerate}
	}

\end{frame}


\begin{frame}{C-ReLU: Mapping back to Non-Convex Networks}

	{\raggedright
		\large
		2. Extend results to \bad{non-convex} ReLU networks
		using the solution mapping.
		\pause
	}
	\horizontalrule

	We need to do some accounting for \bad{model symmetries}:
	\pause
	\begin{itemize}
		\item \textbf{Permutations}: Re-ordering neurons inside the layers.
		      \pause

		\item \textbf{Splits}: Splitting a neuron into two collinear neurons.
		      \pause
	\end{itemize}


	\begin{theorem}[Informal]
		Suppose \( m \geq m^* \).
		Then the optimal set for NC-ReLU up to
		\bad{permutation/split symmetries} is
		\vspace{-1ex}
		\begin{equation*}
			\begin{aligned}
				\hspace{-0.5em} \calO_\lambda  = \,
				\big\{
				 & (W_1,  w_2) :
				\, f_{W_1, w_2}(X)  =  \hat y,                       \\
				 & \forall \, \bi  \in  \calS_\lambda,
				\good{W_{1i} = (\sfrac{\alpha_{i}}{\lambda})^{\sfrac{1}{2}} q_i},
				\good{w_{2i} = (\alpha_i \lambda)^{\sfrac{1}{2}}},
				\alpha_i \geq 0                                      \\
				 & \forall \, \bi  \in [2p] \setminus \calS_\lambda,
				W_{1i} = 0, \, w_{2i} = 0
				\big\}.
			\end{aligned}
		\end{equation*}
	\end{theorem}

\end{frame}

\begin{frame}{C-ReLU: Appearance of Solution Sets}
	\begin{figure}[]
		\centering
		\includegraphics[width=0.96\textwidth]{assets/solution_sets_vis_270.png}
	\end{figure}

	\begin{itemize}
		\item The non-convex parameterization maps the \good{convex polytope} of
		      solutions into a \bad{curved manifold}.
	\end{itemize}


\end{frame}


\begin{frame}{Exploring the Optimal Set}
	\begin{itemize}
		\item Take 10,000 samples from the set of optimal neural networks.
		      \pause
		\item All samples have (i) \textbf{same training accuracy},
		      (ii)~\textbf{same model norm}, but can generalize differently.
		      \pause
	\end{itemize}

	\begin{figure}[t]
		\centering
		\includegraphics[width=0.48\linewidth]{assets/dist_paper_monks-1.pdf}
		\includegraphics[width=0.48\linewidth]{assets/dist_paper_planning.pdf}
	\end{figure}
\end{frame}


\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge III. Pruning
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}


\begin{frame}{Leveraging our Characterization}

	{\Large Proof Roadmap:}
	\vspace{2em}
	\pause

	\begin{enumerate}
		\large
		\item Characterize solutions to the \good{convex reformulation}
		      using strong duality and KKT conditions.
		      \vspace{1ex}

		\item Extend results to \bad{non-convex} ReLU networks
		      using the solution mapping.
		      \vspace{1ex}

		\item \textbf{Leverage explicit characterization of the optimal
			      set for \good{new insights and algorithms}.}
	\end{enumerate}
\end{frame}

\begin{frame}{Optimal Pruning: Polytope of Solutions}

	{\raggedright
		\large
		3. Leverage explicit characterization of the optimal
		set for \good{new insights and algorithms}.
		\pause
	}

	\horizontalrule

	\begin{columns}
		\begin{column}{0.5\textwidth}

			\begin{equation*}
				\begin{aligned}
					 & \solfn(\lambda) =
					\big\{ \theta  : \sum_{i=1}^{2p} D_i X \theta_i = \hat y,      \\
					 & \hspace{5em} \forall \, \bi  \in  \calS_\lambda,
					\theta_i =  \alpha_\bi q_i, \alpha_\bi \geq 0, \,              \\
					 & \hspace{5em} \forall \, j \in [2p] \setminus \calS_\lambda,
					\theta_{j} = 0
					\big\}
				\end{aligned}
			\end{equation*}

			\pause
			The C-ReLU optimal set is a \good{convex polytope}!

		\end{column}
		\begin{column}{0.5\textwidth}
			\pause
			\begin{figure}[c]
				\centering
				\includegraphics[width=0.8\textwidth]{assets/polytope.png}
				\caption{}
				\label{fig:}
			\end{figure}
		\end{column}
	\end{columns}

\end{frame}

\begin{frame}{Optimal Pruning: Vertices}

	\begin{enumerate}
		\item
		      Stack the \( q_i \) vectors into a matrix \( Q =
		      \begin{bmatrix}
			      \vert &        & \vert  \\
			      q_1   & \cdots & q_{2p} \\
			      \vert &        & \vert  \\
		      \end{bmatrix}.
		      \)

		      \pause
		\item
		      The C-ReLU Optimal Set in \( \alpha \) space is then,
		      \begin{equation}
			      \begin{aligned}
				      \solfn(\lambda) & =
				      Q_{\calS_\lambda} \big\{ \alpha \succeq 0  :
				      \sum_{i \in \calS_\lambda} (D_i X q_i) \alpha_i = \hat y,
				      \big\}                                                       \\
				                      & = Q_{\calS_\lambda} \calP_{\calS_\lambda}.
			      \end{aligned}
		      \end{equation}

		      \pause

		\item \( \bar \alpha \in \calP_{\calS_\lambda} \) is a \good{vertex}
		      iff \( \cbr{D_i X q_i}_{\bar \alpha_i \neq 0} \) are linearly independent.
	\end{enumerate}

	\pause
	\horizontalrule

	{\large Are these vertices \good{special} in some way?}

\end{frame}

\begin{frame}{Optimal Pruning: Algorithm}
	\textbf{Definition}: A optimal C-ReLU model \( \theta^* \) is minimal
	if there does
	not exist another optimal model \( \theta' \) with \bad{strictly smaller support}.

	\vspace{3ex}
	\pause

	\begin{beamercolorbox}[wd=\textwidth,sep=1em]{result}
		\textbf{Proposition 3.2} (informal):
		For \( \lambda > 0 \), \( \theta \in \solfn(\lambda) \) is \good{minimal}
		iff
		the vectors \( \cbr{D_i X q_i}_{\alpha_i \neq 0} \)
		are linearly independent.
	\end{beamercolorbox}

	\vspace{3ex}
	\pause

	\textbf{We prove}:
	\begin{itemize}
		\item Vertices of \( \solfn(\lambda) \) are minimal models.
		      \pause
		\item There are at most \( n \) neurons in a minimal model.
		      \pause
		\item Pruning from any optimal model will give a minimal model.
	\end{itemize}

\end{frame}

\begin{frame}{Optimal Pruning: Pseudo-code}
	\begin{algorithm}[H]
		\caption{Pruning solutions}
		\begin{algorithmic}
			\STATE {\bfseries Input:} data matrix \( X \), solution \( \theta \).
			\STATE \( k \gets 0 \).
			\STATE \( \theta^k \gets \theta \).
			\WHILE {\( \exists \beta \neq 0 \) s.t. \( \good{\sum_{\bi \in \act(\theta^k)} \beta_\bi D_i X \theta_i^k = 0} \)}
			\STATE \( \bi^k \gets \argmax_{\bi} \cbr{|\beta_\bi| : \bi \in \act(\theta^k)}  \)
			\STATE \( t^k \gets 1/|\beta_{\bi^k}| \)
			\STATE \( \theta^{k+1} \gets \theta^k (1 - t^k \beta_\bi) \)
			\STATE \( k \gets k + 1 \)
			\ENDWHILE
			\STATE {\bfseries Output:} final weights \( \theta^k \)
		\end{algorithmic}
	\end{algorithm}

	\pause

	Complexity to compute a minimal model starting from \( l \) neurons:

	\[ O\rbr{n^3 l + nd}. \]

\end{frame}


\begin{frame}{(Sub)-Optimal Pruning on UCI}
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.75\textwidth]{assets/uci_pruning_full_paper.pdf}
	\end{figure}
\end{frame}

\begin{frame}{(Sub)-Optimal Pruning on CIFAR-10}
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.75\textwidth]{assets/prune_cifar.pdf}
	\end{figure}
\end{frame}

\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge Recap.
	\end{center}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

\begin{frame}{Recap}
	\begin{center}
		\huge   Our Contributions.
	\end{center}

	\vspace{2em}
	\pause
	{ \large
		\begin{itemize}
			\item \textbf{Optimal Sets}: We derive the set of all optimal
			      two-layer ReLU neural networks.
			      \pause
			      \vspace{0.5em}

			\item \textbf{Regularization Paths}: We have some continuity
			      results (see paper) and are working on more.
			      \pause
			      \vspace{0.5em}

			\item \textbf{Algorithms}: We give a poly-time algorithm for
			      optimally pruning ReLU networks.

		\end{itemize}
	}

\end{frame}


%% main content ends %%

%% end slide
\setbeamercolor{background canvas}{bg=LightCyan}

\begin{frame}{}
	\begin{center}
		\huge Try our Code!
	\end{center}

	\begin{figure}[]
		\centering
		\includegraphics[width=0.6\textwidth]{assets/github.png}
	\end{figure}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

%% bibliography
\begin{frame}[allowframebreaks]{References}
	\printbibliography[]
\end{frame}


\begin{frame}{Bonus: Explicit Optimal Set}
	We gave a characterization of \( \solfn(\lambda) \) that depends on
	\[
		\calS_\lambda
		= \cbr{\bi \in [2p] : \exists \theta \in \solfn(\lambda), \
			\theta_i \neq 0}.
	\]

	Alternative expression involves additional linear constraints.

	\pause
	\horizontalrule

	\begin{equation*}
		\begin{aligned}
			\solfn(\lambda) =
			\big\{ & \theta  :
			\forall \, \bi  \in  \equi,
			\theta_i =  \alpha_\bi q_i, \alpha_\bi \geq 0, \,           \\
			       & \quad \forall \, j \in [2p] \setminus \equi,
			\theta_{j} = 0, \, \sum_{i=1}^{2p} D_i X \theta_i = \hat y, \\
			       & \quad \forall \, i \in [2p],
			K_i \theta_i \geq 0, \abr{\rho, K_i \theta_i } = 0.
			\big\}
		\end{aligned}
	\end{equation*}

	\pause

	More complex, but also \textbf{explicit}.

\end{frame}

\begin{frame}{Bonus: Solution Mapping for C-ReLU}

	Given \( \rbr{v^*, w^*} \), an optimal non-convex ReLU network is given by

	\begin{equation*}
		\textbf{C to NC:} \quad \quad
		\begin{aligned}
			W_{1i} & = v_i^*/ \sqrt{\norm{v_i^*}}, \quad w_{2i} = \sqrt{\norm{v_i^*}}
			\\
			W_{1j} & = w_i^*/ \sqrt{\norm{w_i^*}}, \quad w_{2j} = -\sqrt{\norm{w_i^*}}.
		\end{aligned}
	\end{equation*}

	\pause
	\vspace{3ex}
	\begin{itemize}
		\item Optimal convex weights satisfy \( v_i^* = \alpha_i q_i \)
		      so that
		      \[
			      \norm{v_i^*}_2 = \alpha_i \norm{q_i}_2 = \alpha_i \lambda.
		      \]
	\end{itemize}

	\pause
	\horizontalrule

	Recall structure of \textbf{non-convex optima}:

	\begin{equation*}
		\begin{aligned}
			\hspace{-0.5em} \calO_\lambda  = \,
			\big\{
			 & (W_1,  w_2) :
			\, f_{W_1, w_2}(X)  =  \hat y,                       \\
			 & \forall \, \bi  \in  \calS_\lambda,
			W_{1i} = (\sfrac{\alpha_{i}}{\lambda})^{\sfrac{1}{2}} q_i,
			w_{2i} = (\alpha_i \lambda)^{\sfrac{1}{2}},
			\alpha_i \geq 0                                      \\
			 & \forall \, \bi  \in [2p] \setminus \calS_\lambda,
			W_{1i} = 0, \, w_{2i} = 0
			\big\}.
		\end{aligned}
	\end{equation*}

\end{frame}

\end{document}
